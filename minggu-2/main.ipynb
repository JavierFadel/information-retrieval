{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ef098fe",
   "metadata": {},
   "source": [
    "# \n",
    "# Dokumen ini menjelaskan tentang preprocessing teks yang meliputi beberapa langkah penting seperti mengubah teks menjadi huruf kecil, menghilangkan stopwords, melakukan stemming, dan normalisasi teks.\n",
    "\n",
    " Pada cell pertama, kita membaca file CSV yang berisi tweet dan mengubah semua teks di kolom 'message to examine' menjadi huruf kecil. Kemudian, kita menyimpan 100 baris pertama dari data yang sudah diubah menjadi huruf kecil ke dalam file baru bernama 'lowercased_tweets.csv'.\n",
    "\n",
    " Pada cell ketiga, kita menggunakan NLTK untuk mengunduh dan memuat stopwords dalam bahasa Inggris. Setelah itu, kita menghilangkan stopwords dari setiap pesan di kolom 'message to examine' dan menyimpan hasilnya ke dalam kolom baru bernama 'message_no_stopwords'. Hasilnya ditampilkan dengan menampilkan 5 baris pertama dari dataframe yang sudah dihilangkan stopwords-nya.\n",
    "\n",
    " Pada cell keempat, kita melakukan stemming pada teks yang sudah dihilangkan stopwords-nya. Pertama, kita mengunduh data 'punkt' dari NLTK untuk keperluan tokenisasi. Kemudian, kita melakukan tokenisasi pada teks dan melakukan stemming pada token-token tersebut menggunakan PorterStemmer. Hasil stemming kemudian digabungkan kembali menjadi teks yang sudah dinormalisasi.\n",
    "\n",
    " Secara keseluruhan, dokumen ini memberikan panduan langkah demi langkah untuk melakukan preprocessing teks yang meliputi lower casing, penghilangan stopwords, stemming, dan normalisasi teks menggunakan Python dan NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6425c6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased dataset saved to lowercased_tweets.csv\n",
      "Index,message to examine,label (depression result)\n",
      "\n",
      "106,\"just had a real good moment. i missssssssss him so much, \",0\n",
      "\n",
      "217,is reading manga  http://plurk.com/p/mzp1e,0\n",
      "\n",
      "220,@comeagainjen http://twitpic.com/2y2lx - http://www.youtube.com/watch?v=zogfqvh2me8 ,0\n",
      "\n",
      "288,\"@lapcat need to send 'em to my accountant tomorrow. oddly, i wasn't even referring to my taxes. those are supporting evidence, though. \",0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'sentiment_tweets.csv'\n",
    "\n",
    "# Membaca file CSV\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Mengubah semua teks di kolom 'message to examine' menjadi huruf kecil\n",
    "df['message to examine'] = df['message to examine'].str.lower()\n",
    "\n",
    "# Mengambil 100 baris pertama dari dataframe\n",
    "df_head_100 = df.head(100)\n",
    "\n",
    "# Menyimpan 100 baris pertama yang sudah diubah menjadi huruf kecil ke file baru\n",
    "df_head_100.to_csv('lowercased_tweets.csv', index=False)\n",
    "\n",
    "# Menampilkan pesan bahwa dataset telah disimpan\n",
    "print('Lowercased dataset saved to lowercased_tweets.csv')\n",
    "\n",
    "with open('lowercased_tweets.csv', 'r') as file:\n",
    "    for _ in range(5):\n",
    "        print(file.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64dabfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Index                                 message to examine  \\\n",
      "0    106  just had a real good moment. i missssssssss hi...   \n",
      "1    217         is reading manga  http://plurk.com/p/mzp1e   \n",
      "2    220  @comeagainjen http://twitpic.com/2y2lx - http:...   \n",
      "3    288  @lapcat need to send 'em to my accountant tomo...   \n",
      "4    540      add me on myspace!!!  myspace.com/lookthunder   \n",
      "\n",
      "                                message_no_stopwords  \n",
      "0               real good moment. missssssssss much,  \n",
      "1             reading manga http://plurk.com/p/mzp1e  \n",
      "2  @comeagainjen http://twitpic.com/2y2lx - http:...  \n",
      "3  @lapcat need send 'em accountant tomorrow. odd...  \n",
      "4             add myspace!!! myspace.com/lookthunder  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Langkah 1: Unduh stopwords (jika Anda belum melakukannya)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Langkah 2: Muat dataset Anda dari file CSV\n",
    "df_stopwords = pd.read_csv(file_path)\n",
    "\n",
    "# Langkah 3: Ubah semua teks di kolom 'message to examine' menjadi huruf kecil\n",
    "df_stopwords['message to examine'] = df_stopwords['message to examine'].str.lower()\n",
    "\n",
    "# Langkah 4: Ambil daftar stopwords dalam bahasa Inggris\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Langkah 5: Hilangkan stopwords dari setiap pesan di kolom 'message to examine'\n",
    "df_stopwords['message_no_stopwords'] = df_stopwords['message to examine'].apply(\n",
    "    lambda x: ' '.join(word for word in x.split() if word not in stop_words)\n",
    ")\n",
    "\n",
    "# Langkah 6: Tampilkan 5 baris pertama dari dataframe yang sudah dihilangkan stopwords-nya\n",
    "df_stopwords.head()\n",
    "print(df_stopwords[['Index', 'message to examine', 'message_no_stopwords']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3a5d916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Index                               message_no_stopwords  \\\n",
      "0    106               real good moment. missssssssss much,   \n",
      "1    217             reading manga http://plurk.com/p/mzp1e   \n",
      "2    220  @comeagainjen http://twitpic.com/2y2lx - http:...   \n",
      "3    288  @lapcat need send 'em accountant tomorrow. odd...   \n",
      "4    540             add myspace!!! myspace.com/lookthunder   \n",
      "\n",
      "                                        stemmed_text  \n",
      "0             real good moment . missssssssss much ,  \n",
      "1               read manga http : //plurk.com/p/mzp1  \n",
      "2  @ comeagainjen http : //twitpic.com/2y2lx - ht...  \n",
      "3  @ lapcat need send 'em account tomorrow . oddl...  \n",
      "4             add myspac ! ! ! myspace.com/lookthund  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Mengunduh data 'punkt' dari NLTK\n",
    "nltk.download('punkt')\n",
    "\n",
    "df_stemming = df_stopwords\n",
    "\n",
    "# Tokenisasi teks yang sudah dihilangkan stopwords-nya\n",
    "df_stemming['tokens'] = df_stemming['message_no_stopwords'].apply(nltk.word_tokenize)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Melakukan stemming pada token-token\n",
    "df_stemming['stemmed'] = df_stemming['tokens'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "# Menggabungkan kembali token-token yang sudah di-stem menjadi teks\n",
    "df_stemming['stemmed_text'] = df_stemming['stemmed'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Menampilkan 5 baris pertama dari dataframe yang sudah di-stem\n",
    "print(df_stemming[['Index', 'message_no_stopwords', 'stemmed_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a85b5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Index                                 message to examine  \\\n",
      "0    106  just had a real good moment. i missssssssss hi...   \n",
      "1    217         is reading manga  http://plurk.com/p/mzp1e   \n",
      "2    220  @comeagainjen http://twitpic.com/2y2lx - http:...   \n",
      "3    288  @lapcat need to send 'em to my accountant tomo...   \n",
      "4    540      add me on myspace!!!  myspace.com/lookthunder   \n",
      "\n",
      "                                        stemmed_text  \\\n",
      "0             real good moment . missssssssss much ,   \n",
      "1               read manga http : //plurk.com/p/mzp1   \n",
      "2  @ comeagainjen http : //twitpic.com/2y2lx - ht...   \n",
      "3  @ lapcat need send 'em account tomorrow . oddl...   \n",
      "4             add myspac ! ! ! myspace.com/lookthund   \n",
      "\n",
      "                                     normalized_text  \n",
      "0               real good moment  missssssssss much   \n",
      "1                     read manga http  plurkcompmzp1  \n",
      "2   comeagainjen http  twitpiccom2y2lx  http  www...  \n",
      "3   lapcat need send em account tomorrow  oddli  ...  \n",
      "4                  add myspac    myspacecomlookthund  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Salin df_stemming ke df_normalization\n",
    "df_normalization = df_stemming.copy()\n",
    "\n",
    "# Contoh kamus singkatan (Anda dapat menggantinya dengan yang lebih lengkap)\n",
    "abbreviations = {\n",
    "    \"u\": \"you\",\n",
    "    \"r\": \"are\",\n",
    "    \"c\": \"see\",\n",
    "    \"b4\": \"before\",\n",
    "    \"gtg\": \"got to go\",\n",
    "    \"lol\": \"laugh out loud\",\n",
    "    \"idk\": \"I don't know\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"afk\": \"away from keyboard\",\n",
    "    \"bff\": \"best friends forever\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"irl\": \"in real life\",\n",
    "    \"jk\": \"just kidding\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"rofl\": \"rolling on the floor laughing\",\n",
    "    \"ttyl\": \"talk to you later\",\n",
    "    \"wfh\": \"work from home\",\n",
    "    \"wtf\": \"what the f***\",\n",
    "    \"yw\": \"you're welcome\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"dm\": \"direct message\",\n",
    "    \"tba\": \"to be announced\",\n",
    "    \"tbd\": \"to be determined\",\n",
    "    \"ftw\": \"for the win\",\n",
    "    \"nvm\": \"never mind\",\n",
    "    \"omw\": \"on my way\",\n",
    "    \"ppl\": \"people\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"bday\": \"birthday\",\n",
    "    \"hmu\": \"hit me up\",\n",
    "    \"icymi\": \"in case you missed it\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"lmk\": \"let me know\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"rn\": \"right now\",\n",
    "    \"tmi\": \"too much information\",\n",
    "    \"ttys\": \"talk to you soon\",\n",
    "    \"wbu\": \"what about you\",\n",
    "    \"wyd\": \"what are you doing\",\n",
    "    # Tambahkan lebih banyak singkatan sesuai kebutuhan\n",
    "}\n",
    "\n",
    "# Fungsi untuk normalisasi\n",
    "def normalize_text(text):\n",
    "    # Ubah ke huruf kecil\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Hapus tanda baca dan karakter khusus\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Ganti singkatan umum menggunakan kamus\n",
    "    for abbr, full in abbreviations.items():\n",
    "        text = re.sub(r'\\b' + re.escape(abbr) + r'\\b', full, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Terapkan normalisasi ke kolom 'stemmed_text'\n",
    "df_normalization['normalized_text'] = df_normalization['stemmed_text'].apply(normalize_text)\n",
    "\n",
    "# Tampilkan teks asli yang sudah di-stem dan teks yang sudah dinormalisasi\n",
    "print(df_normalization[['Index', 'message to examine', 'stemmed_text', 'normalized_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96526eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
